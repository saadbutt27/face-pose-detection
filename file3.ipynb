{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00f9d6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saad2\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "# import torch\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "from math import atan2, degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40c8b854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 1 face, 99.8ms\n",
      "Speed: 4.1ms preprocess, 99.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Horizontal Angle: 42.21 degrees\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import mediapipe as mp\n",
    "from math import atan2, degrees\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Initialize YOLOv8 model for face detection\n",
    "def load_yolo_model():\n",
    "    # Load the YOLOv8 model (assuming you have a custom trained model `yolov8n-face-lindevs.pt`)\n",
    "    model = YOLO(\"yolov8n-face-lindevs.pt\")  # Make sure the path to the model is correct\n",
    "    return model\n",
    "\n",
    "# Function to detect faces using YOLOv8\n",
    "def detect_faces_yolo(image, model):\n",
    "    results = model(image)  # Run inference on the image\n",
    "    faces = []\n",
    "    \n",
    "    # YOLOv8 results contain a list of detections\n",
    "    for result in results:\n",
    "        # Each result contains information about the detected object (bounding box, confidence, and class)\n",
    "        boxes = result.boxes  # Access the bounding boxes\n",
    "        \n",
    "        for box in boxes:\n",
    "            xmin, ymin, xmax, ymax = box.xyxy[0].tolist()  # Get bounding box coordinates\n",
    "            conf = box.conf[0].item()  # Get confidence score\n",
    "            cls = box.cls[0].item()  # Get class ID\n",
    "            \n",
    "            if int(cls) == 0:  # Class 0 is 'person' (for face detection)\n",
    "                faces.append([int(xmin), int(ymin), int(xmax), int(ymax)])\n",
    "    \n",
    "    return faces\n",
    "\n",
    "# Function to estimate head pose using Mediapipe and draw landmarks\n",
    "def estimate_head_pose(image, face_landmarks):\n",
    "    # Mediapipe face mesh model\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    face_mesh = mp_face_mesh.FaceMesh(max_num_faces=1)\n",
    "\n",
    "    # Convert image to RGB\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb_image)\n",
    "    \n",
    "    if results.multi_face_landmarks:\n",
    "        # Assuming the first face is the one we are working with\n",
    "        landmarks = results.multi_face_landmarks[0]\n",
    "        \n",
    "        # Loop over landmarks and draw them\n",
    "        for idx, landmark in enumerate(landmarks.landmark):\n",
    "            # Convert landmark from normalized coordinates to pixel coordinates\n",
    "            x, y = int(landmark.x * image.shape[1]), int(landmark.y * image.shape[0])\n",
    "            cv2.circle(image, (x, y), 1, (0, 0, 255), -1)  # Red color for landmarks\n",
    "        \n",
    "        # Get the nose tip and eyes positions (simplified for head pose)\n",
    "        nose = np.array([landmarks.landmark[1].x, landmarks.landmark[1].y])\n",
    "        left_eye = np.array([landmarks.landmark[33].x, landmarks.landmark[33].y])\n",
    "        right_eye = np.array([landmarks.landmark[133].x, landmarks.landmark[133].y])\n",
    "\n",
    "        # Calculate the head pose by analyzing eye and nose positions\n",
    "        horizontal_angle = atan2(nose[1] - (left_eye[1] + right_eye[1]) / 2, nose[0] - (left_eye[0] + right_eye[0]) / 2)\n",
    "        horizontal_angle = degrees(horizontal_angle)\n",
    "        \n",
    "        print(f\"Horizontal Angle: {horizontal_angle:.2f} degrees\")\n",
    "        # Based on angle, decide direction\n",
    "        if horizontal_angle > 30 and horizontal_angle < 40:\n",
    "            head_pose = \"Right\"\n",
    "        elif horizontal_angle < -10:\n",
    "            head_pose = \"Left\"\n",
    "        else:\n",
    "            head_pose = \"Forward\"\n",
    "            \n",
    "        return head_pose\n",
    "    return \"No face detected\"\n",
    "\n",
    "# Main function to process image\n",
    "def main(image_path):\n",
    "    # Load the YOLOv8 model\n",
    "    model = load_yolo_model()\n",
    "\n",
    "    # Read image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Detect faces using YOLO\n",
    "    faces = detect_faces_yolo(image, model)\n",
    "    \n",
    "    if not faces:\n",
    "        print(\"No faces detected.\")\n",
    "        return\n",
    "    \n",
    "    for face in faces:\n",
    "        x1, y1, x2, y2 = map(int, face)  # Get face bounding box coordinates\n",
    "        face_image = image[y1:y2, x1:x2]  # Crop face area from the image\n",
    "        \n",
    "        # Estimate head pose using Mediapipe\n",
    "        head_pose = estimate_head_pose(face_image, None)\n",
    "        \n",
    "        # Draw bounding box and display the head pose\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(image, f\"Head Pose: {head_pose}\", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "    # save the image\n",
    "    cv2.imwrite(\"output.jpg\", image)\n",
    "\n",
    "    # Display image with annotations\n",
    "    cv2.imshow(\"Detected Faces and Head Pose\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = \"./images/1.jpg\"  # Replace with your image file path\n",
    "    main(image_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
